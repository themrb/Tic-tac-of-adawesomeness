\documentclass[10pt]{report}
\usepackage{times}
\usepackage{anysize}
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{amsmath}

\setlength{\headheight}{15.2pt}
\setlength{\headwidth}{495pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}
\pagestyle{fancy}
\rhead{Page \thepage}
\lhead{Warm Up Project}
\marginsize{2cm}{1.5cm}{2cm}{2cm}

\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}

\begin{document}

\title{COMP3130 Warm Up Project}
\date{March 25, 2012}
\author{Jarrah Bloomfield - U4838292, Josh Godsiff - U4685222}
\maketitle

\setlength{\columnsep}{22.0pt}
\begin{multicols}{2}

\section*{\emph{Design Problem}}
\hrule
\vspace{0.4cm}

The problem is to design and implement an intelligent agent to play a variant of tic tac toe, played on a 3D 4x4x4 game space, using the adversarial search algorithm Minimax, extended with alpha beta pruning.

In 4x4x4 tic tac toe, two players (X and O) take turns to place their token in a 3D 4x4x4 grid. A win occurs when one player creates a straight line of four tokens, in any direction.

The state space is around 3 x $10^{30}$. As there are 64 points on the grid, which could be either empty, X or O, the possible states is $3^{64}$ less the number of states that contain more than one winning line that are not all connected through the same point, as such states cannot be reached in legal play.

\section*{\emph{{The Design}}}
\hrule
\vspace{0.4cm}

This design uses Minimax with alpha beta pruning to search the game tree to a certain depth. Smart order of node expansion and immediate terminal checking of child nodes were used in order to increase the effectiveness of the pruning algorithm.

Performance was increased through a minimalist representation of the game state and use of stack memory.

Ada was used to code the game API and the agent.

\section*{\emph{\textmd{Game Representation}}}
\hrule
\vspace{0.4cm}

The game state was represented using two 64 bit binary codes, with each digit representing a different point on the game grid, one for X and O each. This was achieved using a boolean array under Ada's compiler directive $pragma~Packed$. This ensured efficient use of memory and increased computational performance by minimising the main memory load/store instructions required.

\section*{\emph{{Minimax with Alpha Beta Pruning}}}
\hrule
\vspace{0.4cm}

Minimax is an adversarial search algorithm which expands the game tree in a depth first search in order to maximise game value, under the assumption that the opponent has perfect play by performing the same search.

The alpha-beta pruning extension allows us to disregard branches of the game tree that a perfect opponent would not give us a chance to play, significantly reducing the size of the game tree that needs exploring.

\section*{\emph{\textmd{Minimax Disadvantages}}}
\hrule
\vspace{0.4cm}

The disadvantage of Minimax on this domain is the low granularity of the values - namely, -1 for an opponent win, 0 for no win yet and 1 for a win. This makes Minimax completely ignore expediency - it will not necessarily take the quickest route to a forced win, as it sees all winning branches as equal. More devastating it will not necessarily take the longest move to a loss (which against a non-perfect player might result in a recovery); if it sees it would lose against a perfect player, it will see all moves as equally bad and likely pick one which hastens its demise.

\section*{\emph{\textmd{Move Value Maximum and Minimums Optimisation}}}
\hrule
\vspace{0.4cm}

A small but important modification was made to the alpha-beta pruning. $\alpha$ was initialised to -1, and $\beta$ was initialised to 1. This is based on the observation that -1 is the lowest possible value, and 1 is the best possible value. The effect of this is that minimax will take a win immediately, which is valid in this domain as all wins are equal.

\section*{\emph{\textmd{Sucessor Terminal Check Optimisation}}}
\hrule
\vspace{0.4cm}

Before any other computation, Min and Max check if any of the successor states to their current state are terminal; that is, do they have a winning move next move. As in this domain, the value of any win is equal, this value can be immediately returned as it is the best move possible (therefore all other successor states can be pruned). This adds a 'check bredth first' element to the Minimax algorithm but greatly improves performance as it prevents Min and Max from computing deep game trees before realising it is one move from victory. This also helps reduce the horizon problem.

However, the drawback to this optimisation is that the entire set of successors must be computed before we can check if they are terminal, adding a degree of extra computation to each node which might not otherwise have been done.

\section*{\emph{\textmd{Pruning Results}}}
\hrule
\vspace{0.4cm}

\begin{itemize}
\item search = search depth
\item absolute = current absolute level
\item depth = depth from start of minimax
\item states = number of succesors that were pruned
\end{itemize}

$pruned = states * \frac{(64 - absolute)}{(64 - (absolute - depth + search))!}$

*pruning data to go here*

\section*{\emph{{Other Optimisations}}}
\hrule
\vspace{0.4cm}

\section*{\emph{\textmd{Node Expansion Order}}}
\hrule
\vspace{0.4cm}

A simple node expansion order was used in order to direct minimax down the most interesting path (the most likely path of a win) first, in order to maximise the number of branches pruned. It was important to keep this simple, to avoid the overhead from computing the order outweighing its benefit.

The expansion order classified move choices into two categories: one consisting of outer corner cells or inner completely surrounded cells, another of all other cells. The logic behind this is based on the domain observation that cells in the first category can be part of 7 winning lines, whereas cells in the second category can be part of 4 winning lines - meaning in general, playing on cells in the first category is more likely to result in a win. This was a very simple test for a significantly better order of expansion.

\section*{\emph{\textmd{Concurrent Searching}}}
\hrule
\vspace{0.4cm}

The AI attempts to take advantage of any extra available cores to speed up evaluation time. It does this by expanding the current root node - to get its successor states - and then assigning each of these successor nodes to a worker process, which searches the tree below that particular node. When the worker is finished with a node, the results are aggregated with the current best known move, and then the worker is assigned a new node (if any). This continues until all the successors of the root node are exhausted, and thus a best move found.

The concurrency has the effect of producing a linear speed up consistent with the number of extra cores involved. For example, a game where both players expanded to a fixed depth of 7 took approximately 18 minutes on a desktop machine, using only one of its cores. Once 7 of the machine's 8 cores were involved, the time taken for the same game was approximately 4 minutes. The difference between the actual time (4 minutes) and what we would expect given a truely linear speed up ($\frac{18}{7} \approx 2.5$ minutes) is explained by the fact that the first set of nodes evaluated during each move tends to take much more time than subsequent ones, due to the lack of accurate alpha-beta pruning data available at that time.

There are other techniques that could take advantage of multiple cores - for example, maintaining a stack of unevaluated nodes. However, most of these techniques are either considerably more difficult to implement, or involve much more overhead than our chosen technique.

The use of concurrency has the effect of introducing an element of non-determinism, such that the results tend to be slightly different to a purely sequential approach. Both appear to function correctly, and indeed the results for the concurrent implementation seem to be slightly better - possibly due to nodes with a higher likelyhood of a definite result having more pruning occur, and thus being evaluated more quickly.

\section*{\emph{\textmd{Memory Usage}}}
\hrule
\vspace{0.4cm}

Most of the memory is assigned using the stack only, rather than the heap. This increases performance as stack memory is faster than heap memory, due to faster allocation, deallocation and better caching. This also alleviates the need to use Ada's garbage collection to deallocate memory structures. 

The almost exclusive use of the stack is genuinely advantageous (from a programming perspective), given that Ada's automated memory management is on a similar level to 'C' - that is to say, almost non-existent. However, it must be noted that this approach has the drawback of not caching results - a strategy that could potentially save large amounts of computation time.

\section*{\emph{{Perfect Strategy}}}
\hrule
\vspace{0.4cm}

This domain is also known as 'Qubic'. Perfect strategy was mathematically solved by Oren Patashnik in 1980 in \em Mathematics Magazine\em, proving that the first player will win every time under perfect strategy. In 1992 Victor Allis and Patrick Schoo constructed the first agent guaranteed to win if it played first, and caused Qubic to be removed from future Computer Games Olympiad tournaments.

While theoretically this design could play optimally if given enough computation, the computational restrictions force a depth restriction, which  far from reaches perfect strategy. With improvements, and encoding extensive domain knowledge beyond the scope of the warm up project, perfect strategy would be possible to achieve.

The depth cut off enforced in the implementation is 7 nodes deep (not including root node). This was mainly influenced by computation limits (see average timings for test runs). While imperfect, this is deep enough to allow the agent to both enact and counter advanced strategies.

Reference: \\
\em A Perfect Qubic-Playing Program\em, Roberto Waldteufel\\
http://www.wylliedraughts.com/Qubic.htm

\section*{\emph{{Tests Taken}}}
\hrule
\vspace{0.4cm}

The agent was tested by playing itself multiple times, to differing depths.

The time taken to explore the game tree depths varied on different depths chosen. Five tests were taken on depths 5, 6, and 7, on a core 8 i7 processor.

Depth 5 moves took an average of 0.6 seconds each.\\
Depth 6 moves took an average of 3.2 seconds each.\\
Depth 7 moves took an average of 27.2 seconds each.\\

Depth 8 was performed on a core 4 i5 processor, due to overheating causing extreme system instability on the i7.

Depth 8 moves took an average of 251.7 seonds each.

\section*{\emph{\textmd{Alpha-beta speedup}}}
\hrule
\vspace{0.4cm}

\section*{\emph{\textmd{Acknowledgements}}}
\hrule
\vspace{0.4cm}

Uwe Zimmer for use of his exception creation and handling code in Ada.

\end{multicols}
\end{document}